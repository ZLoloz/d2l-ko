# 선형 회귀 분석
:label:`sec_linear_regression`

*회귀*는 모델링을위한 일련의 방법을 나타냅니다.
하나 이상의 독립 변수와 종속 변수 사이의 관계.자연 과학 및 사회 과학에서 회귀의 목적은 가장 자주
*입력과 출력 사이의 관계를 특성화*.
반면에 기계 학습은 대부분 *예측*과 관련이 있습니다.

회귀 문제는 우리가 숫자 값을 예측 할 때마다 나타납니다.일반적인 예로는 주택, 주식 등의 가격 예측, 체류 기간 예측 (병원 환자), 수요 예측 (소매 판매) 등이 있습니다.모든 예측 문제가 고전적인 회귀 문제가 아닙니다.후속 섹션에서, 우리는 목표가 범주의 집합 중 회원을 예측하는 분류 문제를 소개합니다.

## 선형 회귀 분석의 기본 요소

*선형 회귀*는 둘 다 가장 간단 할 수 있습니다.
회귀 분석에 대한 표준 도구 중에서 가장 인기가 있습니다.19 세기의 새벽까지 거슬러 올라가는 선형 회귀는 몇 가지 간단한 가정에서 흘러 나옵니다.첫째, 우리는 독립 변수 $\mathbf{x}$와 종속 변수 $y$ 사이의 관계가 선형이라고 가정합니다. 즉, $y$는 $\mathbf{x}$의 원소의 가중 합으로 표현 될 수 있습니다 관측에 약간의 잡음.둘째, 우리는 어떤 노이즈가 잘 동작한다고 가정합니다 (가우스 분포를 따름).

접근 방식에 동기를 부여하기 위해, 우리가 실행 예제로 시작하자.면적 (평방 피트) 과 나이 (년) 를 기준으로 주택 가격 (달러 단위) 을 추정하려고한다고 가정합니다.주택 가격을 예측하기위한 모델을 실제로 맞추기 위해서는 각 주택의 판매 가격, 면적 및 나이를 알고있는 판매로 구성된 데이터 세트를 손에 넣어야합니다.기계 학습 용어에서 데이터 집합은*교육 데이터 세트* 또는*교육 집합*이라고 하며 각 행 (여기서 하나의 판매에 해당하는 데이터) 을*example* (또는*데이터 포인트*, *데이터 인스턴스*, *샘플*) 이라고합니다.우리가 예측하려고하는 것 (가격) 을* 라벨* (또는* 대상*) 이라고합니다.예측의 기반이 되는 독립 변수 (연령 및 면적) 를 *피쳐* (또는*공변량*) 라고 합니다.

일반적으로 $n$을 사용하여 데이터 세트의 예제 수를 나타냅니다.각 입력을 $\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\top$로 지정하고 해당 레이블을 $y^{(i)}$으로 나타내는 7323615로 데이터 요소를 색인화합니다.

### 선형 모형
:label:`subsec_linear_model`

선형성 가정은 목표 (가격) 가 피처 (면적 및 연령) 의 가중 합으로 표현 될 수 있다고 말합니다.

$$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.$$
:eqlabel:`eq_price-area`

:eqref:`eq_price-area`에서는 $w_{\mathrm{area}}$ 및 $w_{\mathrm{age}}$를 *가중치*라고 하며, $b$은 *바이어스*라고 부릅니다 (*오프셋* 또는 *가로채기*라고도 함).가중치는 각 피처가 예측에 미치는 영향을 결정하며 편향은 모든 피처가 0 값을 가질 때 예측 가격이 어떤 값을 가져야하는지 말합니다.우리가 제로 지역이있는 집을 결코 보지 못하거나 정확하게 0 년 된 주택을 보지 않더라도 우리는 여전히 편견이 필요합니다. 그렇지 않으면 우리 모델의 표현력을 제한 할 것입니다.엄밀히 말하면, :eqref:`eq_price-area`는 입력 피처의*아핀 변환*이며, 가중 합계를 통한 피처의*선형 변환*과 추가 된 바이어스를 통해*변환*과 결합되어 특징입니다.

데이터 세트를 감안할 때 우리의 목표는 가중치 $\mathbf{w}$와 편향 $b$를 선택하여 평균적으로 모델에 따른 예측이 데이터에서 관찰된 실제 가격에 가장 적합하도록 하는 것입니다.출력 예측이 입력 피처의 아핀 변환에 의해 결정되는 모델은*선형 모델*이며, 아핀 변환은 선택된 가중치와 치우침에 의해 지정됩니다.

몇 가지 기능만으로 데이터 세트에 집중하는 것이 일반적인 분야에서는 이와 같이 긴 형식의 모델을 명시 적으로 표현하는 것이 일반적입니다.기계 학습에서는 일반적으로 고차원 데이터 세트로 작업하므로 선형 대수 표기법을 사용하는 것이 더 편리합니다.입력이 $d$ 기능으로 구성되면 $\hat{y}$ (일반적으로 “모자”기호는 추정치를 나타냄) 를 다음과 같이 표현합니다.

$$\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$

모든 기능을 벡터 $\mathbf{x} \in \mathbb{R}^d$로 수집하고 모든 가중치를 벡터 $\mathbf{w} \in \mathbb{R}^d$로 수집하면 내적을 사용하여 모델을 콤팩트하게 표현할 수 있습니다.

$$\hat{y} = \mathbf{w}^\top \mathbf{x} + b.$$
:eqlabel:`eq_linreg-y`

:eqref:`eq_linreg-y`에서 벡터 $\mathbf{x}$는 단일 데이터 요소의 피쳐에 해당합니다.우리는 종종 디자인 매트릭스* $\mathbf{X} \in \mathbb{R}^{n \times d}$을 통해 $n$ 예제의 전체 데이터 세트의 기능을 참조하는 것이 편리하다는 것을 알게 될 것입니다.여기서 $\mathbf{X}$은 모든 예제에 대해 하나의 행과 모든 피처에 대해 하나의 열을 포함합니다.

$\mathbf{X}$ 기능 모음의 경우 $\hat{\mathbf{y}} \in \mathbb{R}^n$는 행렬 벡터 제품을 통해 표현할 수 있습니다.

$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$

여기서 방송 (:numref:`subsec_broadcasting` 참조) 이 합산 중에 적용됩니다.교육 데이터 세트 $\mathbf{X}$ 및 해당 (알려진) 레이블 $\mathbf{y}$의 기능을 감안할 때 선형 회귀의 목표는 $\mathbf{w}$과 동일한 분포에서 샘플링 된 새로운 데이터 포인트의 기능을 제공하는 가중치 벡터 $\mathbf{w}$과 바이어스 용어 $b$를 찾는 것입니다. 새 데이터 포인트의 레이블은 (기대) 가 가장 낮은 오차 예측 될 수 있습니다.

우리는 $\mathbf{x}$ 주어진 $y$을 예측하기위한 최상의 모델이 선형이라고 생각하더라도, 우리는 $y^{(i)}$이 정확히 $\mathbf{w}^\top \mathbf{x}^{(i)}+b$과 같은 $n$의 실제 데이터 세트를 찾을 것으로 예상하지 않을 것입니다 모든 $1 \leq i \leq n$.예를 들어, 우리가 기능 $\mathbf{X}$ 및 라벨 $\mathbf{y}$을 관찰하기 위해 사용하는 장비는 측정 오차의 작은 금액을 겪을 수 있습니다.따라서 기본 관계가 선형이라고 확신하는 경우에도 이러한 오류를 설명하기 위해 노이즈 용어를 통합할 것입니다.

최상의 매개 변수* (또는*model 매개 변수*) $\mathbf{w}$ 및 $b$를 검색하기 전에 두 가지 더 필요합니다. (i) 특정 모델의 품질 측정; (ii) 품질을 향상시키기 위해 모델을 업데이트하는 절차.

### 손실 함수

우리의 모델에 어떻게 맞는지*에 대해 생각하기 전에, 우리는*적합성*의 척도를 결정해야합니다.*손실 함수*는 대상의*실수*및*예측* 값 사이의 거리를 정량화합니다.손실은 일반적으로 작은 값이 더 좋고 완벽한 예측으로 0의 손실이 발생하는 음수가 아닌 숫자입니다.회귀 문제에서 가장 많이 사용되는 손실 함수는 제곱 오류입니다.예제 $i$에 대한 예측이 $\hat{y}^{(i)}$이고 해당 참 레이블이 $y^{(i)}$인 경우 제곱 오차는 다음과 같이 계산됩니다.

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$

상수 $\frac{1}{2}$는 실제 차이를 만들지 않지만 표기적으로 편리함을 증명할 것이며, 손실의 파생물을 취할 때 취소됩니다.훈련 데이터 세트가 우리에게 주어지기 때문에 우리의 통제 불능, 경험적 오류는 모델 매개 변수의 함수 일뿐입니다.좀 더 구체적으로 만들려면 :numref:`fig_fit_linreg`에 표시된 것처럼 1 차원 사례에 대한 회귀 문제를 플롯하는 아래 예제를 고려하십시오.

![Fit data with a linear model.](../img/fit_linreg.svg)
:label:`fig_fit_linreg`

추정치 $\hat{y}^{(i)}$과 관측치 $y^{(i)}$ 간의 차이가 크면 2차 의존성으로 인해 손실에 대한 기여도가 더 커집니다.$n$ 예제의 전체 데이터 집합에서 모델의 품질을 측정하기 위해 학습 집합의 손실을 평균 (또는 동등하게 합계) 합니다.

$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

모델을 학습할 때 모든 교육 예제에서 총 손실을 최소화하는 매개 변수 ($\mathbf{w}^*, b^*$) 를 찾고 싶습니다.

$$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).$$

### 분석 솔루션

선형 회귀는 비정상적으로 간단한 최적화 문제입니다.우리는이 책에서 발생합니다 대부분의 다른 모델과는 달리, 선형 회귀 분석은 간단한 수식을 적용하여 분석적으로 해결 될 수있다.시작하려면 모든 것으로 구성된 설계 행렬에 열을 추가하여 바이어스 $b$을 매개 변수 7323615에 합산 할 수 있습니다.그런 다음 우리의 예측 문제는 $\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$를 최소화하는 것입니다.손실 표면에는 단 하나의 중요한 지점이 있으며 전체 도메인에 대한 손실의 최소에 해당합니다.$\mathbf{w}$와 관련하여 손실의 파생물을 가져 와서 0으로 설정하면 분석 (폐쇄 형) 솔루션이 산출됩니다.

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.$$

선형 회귀 분석과 같은 간단한 문제는 분석 솔루션을 인정할 수 있지만 그러한 행운에 익숙해서는 안됩니다.분석 솔루션은 훌륭한 수학적 분석을 허용하지만 분석 솔루션의 요구 사항은 매우 제한적이어서 모든 딥 러닝을 제외할 수 있습니다.

### 미니배치 확률 그라데이션 하강

분석적으로 모델을 해결할 수없는 경우에도 실제로 모델을 효과적으로 훈련 할 수 있습니다.더욱이 많은 작업에서 최적화하기 어려운 모델이 훨씬 더 좋기 때문에 훈련하는 방법을 알아내는 것이 문제가 될 가치가 있습니다.

거의 모든 딥 러닝 모델을 최적화하는 핵심 기법이며 이 책에서 살펴볼 내용은 손실 함수를 점진적으로 낮추는 방향으로 매개 변수를 업데이트하여 오류를 반복적으로 줄이는 것입니다.이 알고리즘은*그라데이션 하위*라고합니다.

그라디언트 하강의 가장 순진한 응용 프로그램은 데이터 세트의 모든 단일 예제에서 계산 된 손실의 평균 인 손실 함수의 미분을 취하는 것으로 구성됩니다.실제로 이것은 매우 느릴 수 있습니다. 단일 업데이트를하기 전에 전체 데이터 집합을 전달해야합니다.따라서 우리는 종종 업데이트를 계산할 필요가있을 때마다 무작위 미니 배치를 샘플링하기 위해 정착 할 것입니다. 이 변형은* 미니 배치 확률 적 그래디언트 하강*입니다.

각 반복에서, 우리는 먼저 고정 된 수의 교육 예제로 구성된 미니 배치 $\mathcal{B}$를 무작위로 샘플링합니다.그런 다음 모델 매개 변수와 관련하여 미니 배치의 평균 손실의 미분 (그라데이션) 을 계산합니다.마지막으로 그라디언트에 미리 결정된 양수 값 $\eta$를 곱하고 결과 용어를 현재 매개 변수 값에서 뺍니다.

다음과 같이 수학적으로 업데이트를 표현할 수 있습니다 ($\partial$는 부분 파생물을 나타냅니다).

$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

요약하면 알고리즘의 단계는 다음과 같습니다. (i) 모델 매개 변수의 값을 일반적으로 무작위로 초기화합니다. (ii) 데이터에서 무작위 미니 배치를 반복적으로 샘플링하여 음수 그래디언트 방향으로 매개 변수를 업데이트합니다.2 차 손실 및 아핀 변환의 경우 다음과 같이 명시 적으로 작성할 수 있습니다.

$$\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}$$
:eqlabel:`eq_linreg_batch_update`

$\mathbf{w}$과 $\mathbf{x}$은 :eqref:`eq_linreg_batch_update`의 벡터입니다.여기서보다 우아한 벡터 표기법은 계수 측면에서 사물을 표현하는 것보다 수학을 훨씬 읽기 쉽게 만듭니다 (예: $w_1, w_2, \ldots, w_d$).집합 카디널리티 $|\mathcal{B}|$은 각 미니일괄 처리 (*배치 크기*) 의 예제 수를 나타내고 $\eta$는*학습 속도*를 나타냅니다.배치 크기 및 학습 속도 값은 수동으로 미리 지정되며 일반적으로 모델 교육을 통해 학습되지 않는다는 점을 강조합니다.조정 가능하지만 트레이닝 루프에서 업데이트되지 않는 이러한 매개 변수를 *하이퍼 매개 변수*라고 합니다.
*하이퍼 매개 변수 조정*은 하이퍼 매개 변수가 선택되는 과정이며,
에 포함되며 일반적으로 별도의 *유효성 검사 데이터세트* (또는*검증 세트*) 에서 평가된 대로 교육 루프의 결과에 따라 조정해야 합니다.

미리 결정된 반복 횟수에 대한 교육 후 (또는 다른 중지 기준이 충족 될 때까지) $\hat{\mathbf{w}}, \hat{b}$로 표시된 예상 모델 매개 변수를 기록합니다.우리의 함수가 진정으로 선형이고 잡음이 없더라도 알고리즘이 최소기로 천천히 수렴하지만 유한 단계에서 정확하게 달성 할 수 없기 때문에 이러한 매개 변수는 손실의 정확한 최소화가 아닙니다.

선형 회귀는 전체 도메인에 대해 단 하나의 최소값이있는 학습 문제입니다.그러나 깊은 네트워크와 같이 더 복잡한 모델의 경우 손실 표면에는 많은 최소값이 포함됩니다.다행히도 아직 완전히 이해되지 않은 이유 때문에 딥 러닝 실무자는 교육 세트*에서 손실을 최소화하는 매개 변수를 찾는 데 어려움을 겪지 않습니다.더 강력한 작업은 이전에 보지 못했던 데이터에 대한 낮은 손실을 달성하는 매개 변수를 찾는 것입니다. *일반화*라는 도전입니다.우리는 책 전체에서 이러한 주제로 돌아갑니다.

### 학습된 모델을 사용하여 예측 만들기

학습 된 선형 회귀 모델 $\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}$를 감안할 때, 우리는 이제 $x_1$ 및 $x_2$ 연령의 새 주택 (교육 데이터에 포함되지 않음) 의 가격을 추정 할 수 있습니다.주어진 기능을 대상으로 추정하는 것은 일반적으로*예측* 또는*추론*라고합니다.

딥 러닝에서 표준 전문 용어로 등장했음에도 불구하고 이 단계*추론*을 호출하기 때문에*예측*을 고수하려고 노력할 것입니다.통계에서*추론*은 데이터 집합을 기반으로 매개 변수를 추정하는 경우가 더 많습니다.이러한 용어의 오용은 딥 러닝 실무자가 통계학자와 이야기할 때 혼란의 원인입니다.

## 속도에 대한 벡터화

모델을 교육 할 때 일반적으로 전체 미니 배치를 동시에 처리하려고합니다.이를 효율적으로 수행하려면 파이썬에서 값 비싼 for-loops를 작성하는 대신 계산을 벡터화하고 빠른 선형 대수 라이브러리를 활용해야합니다.

```{.python .input}
%matplotlib inline
from d2l import mxnet as d2l
import math
from mxnet import np
import time
```

```{.python .input}
#@tab pytorch
%matplotlib inline
from d2l import torch as d2l
import math
import torch
import numpy as np
import time
```

```{.python .input}
#@tab tensorflow
%matplotlib inline
from d2l import tensorflow as d2l
import math
import tensorflow as tf
import numpy as np
import time
```

이것이 왜 중요한지 설명하기 위해 벡터를 추가하는 두 가지 방법을 고려할 수 있습니다.시작하려면 모든 것을 포함하는 두 개의 10000 차원 벡터를 인스턴스화합니다.한 가지 방법에서 우리는 파이썬 for 루프로 벡터를 반복합니다.다른 방법에서는 `+`에 대한 단일 호출에 의존 할 것입니다.

```{.python .input}
#@tab all
n = 10000
a = d2l.ones(n)
b = d2l.ones(n)
```

이 책에서 실행 시간을 자주 벤치 마크 할 것이므로 타이머를 정의 할 수 있습니다.

```{.python .input}
#@tab all
class Timer:  #@save
    """Record multiple running times."""
    def __init__(self):
        self.times = []
        self.start()

    def start(self):
        """Start the timer."""
        self.tik = time.time()

    def stop(self):
        """Stop the timer and record the time in a list."""
        self.times.append(time.time() - self.tik)
        return self.times[-1]

    def avg(self):
        """Return the average time."""
        return sum(self.times) / len(self.times)

    def sum(self):
        """Return the sum of time."""
        return sum(self.times)

    def cumsum(self):
        """Return the accumulated time."""
        return np.array(self.times).cumsum().tolist()
```

이제 워크로드를 벤치마킹할 수 있습니다.먼저 for-loop를 사용하여 한 번에 하나의 좌표를 추가합니다.

```{.python .input}
#@tab mxnet, pytorch
c = d2l.zeros(n)
timer = Timer()
for i in range(n):
    c[i] = a[i] + b[i]
f'{timer.stop():.5f} sec'
```

```{.python .input}
#@tab tensorflow
c = tf.Variable(d2l.zeros(n))
timer = Timer()
for i in range(n):
    c[i].assign(a[i] + b[i])
f'{timer.stop():.5f} sec'
```

또는 다시로드 된 `+` 연산자를 사용하여 요소별 합계를 계산합니다.

```{.python .input}
#@tab all
timer.start()
d = a + b
f'{timer.stop():.5f} sec'
```

두 번째 방법이 첫 번째 방법보다 훨씬 빠르다는 것을 알았을 것입니다.벡터화 코드는 종종 크기 순서의 속도 향상을 산출합니다.더욱이, 우리는 더 많은 수학을 도서관에 밀어 넣고 많은 계산을 직접 작성할 필요가 없으므로 오류의 가능성을 줄입니다.

## 정규 분포 및 제곱 손실
:label:`subsec_normal_distribution_and_squared_loss`

위의 정보만을 사용하여 이미 손이 더러워 질 수 있지만 다음에서는 잡음 분포에 대한 가정을 통해 제곱 손실 목표에 더 공식적으로 동기를 부여 할 수 있습니다.

선형 회귀 분석은 1795년 가우스에 의해 발명되었으며, 이들은 정규 분포 (*가우시안*이라고도 함) 를 발견했습니다.정규 분포와 선형 회귀 간의 연결이 일반적인 종속 관계보다 더 깊다는 것이 밝혀졌습니다.메모리를 새로 고치기 위해 평균 $\mu$와 분산이 $\sigma^2$ (표준 편차 $\sigma$) 인 정규 분포의 확률 밀도는 다음과 같이 주어집니다.

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$

아래에서 우리는 정규 분포를 계산하는 파이썬 함수를 정의합니다.

```{.python .input}
#@tab all
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

이제 정규 분포를 시각화할 수 있습니다.

```{.python .input}
#@tab all
# Use numpy again for visualization
x = np.arange(-7, 7, 0.01)

# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```

우리가 볼 수 있듯이, 평균을 변경하면 $x$ 축을 따른 이동에 해당하고, 분산을 증가시키면 분포가 분산되어 피크가 낮아집니다.

평균 오차 손실 함수 (또는 단순히 제곱 손실) 를 사용하여 선형 회귀 분석을 동기를 부여하는 한 가지 방법은 노이즈가 일반적으로 다음과 같이 분포되는 잡음이 있는 관측에서 관측치가 발생한다고 공식적으로 가정하는 것입니다.

$$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).$$

따라서, 우리는 지금 통해 주어진 $\mathbf{x}$에 대한 특정 7323615를 보는 것의* 같은 것*을 쓸 수 있습니다

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).$$

이제 최대 우도의 원칙에 따라 매개 변수 $\mathbf{w}$ 및 $b$의 최상의 값은 전체 데이터 세트의*와 같은 값을 최대화하는 것입니다.

$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).$$

최대 우도의 원칙에 따라 선택된 추정기는*최대우도 추정기*라고합니다.많은 지수 함수의 곱을 극대화하는 것이 어려워 보일 수도 있지만, 대신 가능성의 로그를 최대화하여 목표를 변경하지 않고 크게 단순화 할 수 있습니다.역사적인 이유로 최적화는 최대화보다는 최소화로 표현되는 경우가 더 많습니다.따라서 아무 것도 변경하지 않고* 음의 로그와 같은 리후드* $-\log P(\mathbf y \mid \mathbf X)$를 최소화 할 수 있습니다.수학을 연구하면 다음과 같은 결과를 얻을 수 있습니다.

$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$

이제 $\sigma$가 고정 상수라는 가정이 하나 더 필요합니다.따라서 $\mathbf{w}$ 또는 $b$에 의존하지 않기 때문에 첫 번째 용어를 무시할 수 있습니다.이제 두 번째 항은 승법 상수 $\frac{1}{\sigma^2}$를 제외하고 이전에 도입 된 오차 손실 제곱과 동일합니다.다행히도이 솔루션은 $\sigma$에 의존하지 않습니다.따라서 평균 제곱 오차를 최소화하는 것은 가우스 잡음 가정의 선형 모형의 최대우도 추정과 같습니다.

## 선형 회귀 분석에서 딥 네트워크로

지금까지 우리는 선형 모델에 대해서만 이야기했습니다.신경망은 훨씬 더 풍부한 모델 제품군을 포함하지만 선형 모델을 신경망의 언어로 표현함으로써 신경망으로 생각할 수 있습니다.시작하려면 “레이어”표기법으로 물건을 다시 작성하십시오.

### 신경망 다이어그램

딥 러닝 실무자는 다이어그램을 그려서 모델에서 일어나는 일을 시각화하려고 합니다.:numref:`fig_single_neuron`에서 선형 회귀 모델을 신경망으로 묘사합니다.이 다이어그램은 각 입력이 출력에 연결되는 방법과 같은 연결 패턴을 강조하지만 가중치나 바이어스에 의해 취해진 값은 강조하지 않습니다.

![Linear regression is a single-layer neural network.](../img/singleneuron.svg)
:label:`fig_single_neuron`

:numref:`fig_single_neuron`에 표시된 신경망의 경우 입력은 $x_1, \ldots, x_d$이므로 입력 층의* 입력 수* (또는*피쳐 치수*) 는 $d$입니다.:numref:`fig_single_neuron`의 네트워크 출력은 $o_1$이므로 출력 계층의* 출력 수*는 1입니다.입력 값은 모두*주어진*이며 단 하나의*계산된* 뉴런이 있습니다.계산이 일어나는 위치에 초점을 맞추고, 일반적으로 레이어를 계산할 때 입력 레이어를 고려하지 않습니다.즉, :numref:`fig_single_neuron`의 신경망에 대한*층 수*는 1입니다.우리는 선형 회귀 모델을 하나의 인공 뉴런으로 구성된 신경망 또는 단층 신경망으로 생각할 수 있습니다.

선형 회귀의 경우 모든 입력이 모든 출력에 연결되기 때문에 (이 경우 하나의 출력 만 있음) 이 변환 (:numref:`fig_single_neuron`의 출력 레이어) 을* 완전히 연결된 레이어* 또는* 조밀 한 레이어*로 간주 할 수 있습니다.우리는 다음 장에서 이러한 레이어로 구성된 네트워크에 대해 더 많이 이야기 할 것입니다.

### 생물학

선형 회귀 (1795 년에 발명 됨) 는 계산 신경 과학보다 앞선 것이므로 선형 회귀를 신경망으로 설명하는 것은 시대 착오적 인 것처럼 보일 수 있습니다.선형 모델은 사이버 신경학자/신경 생리 학자 워렌 McCulloch와 월터 피츠가 인공 뉴런의 모델을 개발하기 시작했을 때 시작하는 자연스러운 장소 인 이유를 확인하려면 :numref:`fig_Neuron`에서 생물학적 뉴런의 만화 같은 그림을 고려
*수상 돌기* (입력 단자),
*핵* (CPU), *축삭* (출력 와이어), *축삭 단자* (출력 단자), *시냅스*를 통해 다른 뉴런에 연결할 수 있습니다.

![The real neuron.](../img/Neuron.svg)
:label:`fig_Neuron`

다른 뉴런 (또는 망막과 같은 환경 센서) 에서 도착한 정보 $x_i$는 수상 돌기에서 수신됩니다.특히, 이 정보는 입력의 효과를 결정하는*시냅스 가중치* $w_i$ (예: 제품 $x_i w_i$을 통한 활성화 또는 억제) 에 의해 가중됩니다.여러 소스에서 도착하는 가중 입력은 가중 합계 $y = \sum_i x_i w_i + b$로 핵에 집계되며, 이 정보는 일반적으로 $\sigma(y)$를 통해 일부 비선형 처리 후 축삭 $y$에서 추가 처리를 위해 전송됩니다.거기에서 목적지 (예: 근육) 에 도달하거나 수상 돌기를 통해 다른 뉴런으로 공급됩니다.

물론, 그러한 많은 단위가 올바른 연결성과 올바른 학습 알고리즘과 함께 자갈이 될 수 있다는 높은 수준의 생각은 어떤 뉴런만으로도 실제 생물학적 신경계에 대한 우리의 연구에 빚을 표현할 수 있는 것보다 훨씬 더 흥미롭고 복잡한 행동을 만들어냅니다.

동시에, 오늘날 딥 러닝에 대한 대부분의 연구는 신경과학에서 직접적인 영감을 거의 얻지 않습니다.우리는 스튜어트 러셀과 피터 노빅을 호출 누가, 그들의 고전적인 인공 지능 교재에서
*분류: 인공지능
는 비행기가 새들의 영감*을 받았을 수도 있지만, 조류학은 몇 세기 동안 항공 혁신의 주요 동력자가 아니었다고 지적했다.마찬가지로, 요즘 딥 러닝의 영감은 수학, 통계, 컴퓨터 과학에서 동등하거나 더 큰 척도로 나옵니다.

## 요약

* 기계 학습 모델의 핵심 요소는 학습 데이터, 손실 함수, 최적화 알고리즘 및 모델 자체입니다.
* 벡터화는 모든 것을 더 좋게 (대부분 수학) 하고 빠르게 (대부분 코드) 만듭니다.
* 객관적인 기능을 최소화하고 최대 우도 추정을 수행하는 것은 동일한 것을 의미 할 수 있습니다.
* 선형 회귀 모델도 신경망입니다.

## 연습 문제

1. 우리는 몇 가지 데이터가 있다고 가정 $x_1, \ldots, x_n \in \mathbb{R}$.우리의 목표는 $\sum_i (x_i - b)^2$가 최소화되도록 상수 $b$을 찾는 것입니다.
    * 최적의 $b$를 위한 분석 솔루션을 찾아보십시오.
    * 이 문제와 그 해결책은 정규 분포와 어떤 관련이 있습니까?
1. 제곱 오차를 사용한 선형 회귀 분석에 대한 최적화 문제에 대한 분석 솔루션을 파생시킵니다.일을 단순하게 유지하기 위해 문제에서 바이어스 $b$를 생략 할 수 있습니다 (모든 열로 구성된 $\mathbf X$에 하나의 열을 추가하여이 작업을 원칙적으로 수행 할 수 있습니다).
    * 행렬 및 벡터 표기법으로 최적화 문제를 작성하십시오 (모든 데이터를 단일 행렬로 처리하고 모든 대상 값을 단일 벡터로 처리).
    * $w$에 대한 손실의 그라데이션을 계산합니다.
    * 그라데이션을 0으로 설정하고 행렬 방정식을 풀어 분석 솔루션을 찾습니다.
    * 확률 적 그래디언트 강하를 사용하는 것보다 언제 더 좋을까요?이 방법은 언제 깨질 수 있습니까?
1. 가산 잡음 $\epsilon$를 관리하는 잡음 모형이 지수 분포라고 가정합니다.즉, $p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$입니다.
    * 모형 $-\log P(\mathbf y \mid \mathbf X)$ 아래에 있는 데이터의 음수 로그 우도를 작성합니다.
    * 폐쇄 형 솔루션을 찾을 수 있습니까?
    * 이 문제를 해결하기 위해 확률 적 그래디언트 하강 알고리즘을 제안하십시오.무엇이 잘못 될 수 있습니까 (힌트: 매개 변수를 계속 업데이트 할 때 고정 된 지점 근처에서 어떻게됩니까)?이걸 고칠 수 있니?

:begin_tab:`mxnet`
[Discussions](https://discuss.d2l.ai/t/40)
:end_tab:

:begin_tab:`pytorch`
[Discussions](https://discuss.d2l.ai/t/258)
:end_tab:

:begin_tab:`tensorflow`
[Discussions](https://discuss.d2l.ai/t/259)
:end_tab:
