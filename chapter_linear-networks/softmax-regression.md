# 소프트맥스 회귀 분석
:label:`sec_softmax`

:numref:`sec_linear_regression`에서는 :numref:`sec_linear_scratch`에서 처음부터 구현을 거쳐 작업하고 :numref:`sec_linear_concise`의 딥 러닝 프레임워크의 고급 API를 사용하여 무거운 작업을 수행하는 선형 회귀를 도입했습니다.

회귀는 우리가 대답 할 때 도달하는 망치입니다* 얼마입니까?* 또는*얼마나 많은?* 질문.집을 판매 할 달러 (가격) 수 또는 야구 팀이 가질 수있는 승리 수 또는 퇴원하기 전에 환자가 입원 할 수있는 일 수를 예측하려면 아마도 회귀 모델을 찾고 있습니다.

실제로 우리는* 분류*에 더 자주 관심이 있습니다. “얼마나 많은”것이 아니라 “어느 것”을 묻습니다.

* 이 이메일이 스팸 폴더 또는 받은 편지함에 속해 있습니까?
* 이 고객이 가입 서비스에 가입* 가능성* 또는 *가입하지 않을 가능성이 더 높습니까?
* 이 이미지는 당나귀, 개, 고양이 또는 수탉을 묘사합니까?
* Aston이 다음에 시청할 가능성이 가장 높은 영화는 무엇입니까?

구어체로 기계 학습 실무자는 두 가지 미묘하게 다른 문제를 설명하기 위해* 분류*라는 단어를 과부하합니다. (i) 범주 (클래스) 에 대한 예제의 하드 할당에만 관심이있는 곳; (ii) 우리가 소프트 과제를 만들려는 곳, 즉각 카테고리가 적용됩니다.구별은 부분적으로 흐려지는 경향이 있습니다. 왜냐하면 종종 하드 과제에 대해서만 신경 쓰더라도 소프트 할당을 만드는 모델을 사용하기 때문입니다.

## 분류 문제
:label:`subsec_classification-problem`

발을 젖게하려면 간단한 이미지 분류 문제로 시작합시다.여기서 각 입력은 $2\times2$ 회색 음영 이미지로 구성됩니다.우리는 우리에게 네 가지 기능을 제공, 하나의 스칼라 각 픽셀 값을 나타낼 수 $x_1, x_2, x_3, x_4$.또한 각 이미지가 “고양이”, “닭”및 “개”범주 중 하나에 속한다고 가정 해 봅시다.

다음으로, 우리는 라벨을 표현하는 방법을 선택해야합니다.우리는 두 가지 확실한 선택을 가지고 있습니다.아마도 가장 자연스러운 충동은 $y \in \{1, 2, 3\}$를 선택하는 것입니다. 여기서 정수는 각각 $\{\text{dog}, \text{cat}, \text{chicken}\}$를 나타냅니다.이것은 컴퓨터에 이러한 정보를*저장*하는 좋은 방법입니다.카테고리에 자연스러운 순서가 있다면 $\{\text{baby}, \text{toddler}, \text{adolescent}, \text{young adult}, \text{adult}, \text{geriatric}\}$을 예측하려고한다면, 이 문제를 회귀로 변환하고이 형식으로 레이블을 유지하는 것이 좋습니다.

그러나 일반적인 분류 문제는 수업 중 자연스러운 질서와 함께 오지 않습니다.다행히도 통계 학자들은 오래 전에 범주 형 데이터를 나타내는 간단한 방법을 발명했습니다: * 단일 핫 인코딩*.단일 핫 인코딩은 카테고리가있는만큼 많은 구성 요소가있는 벡터입니다.특정 인스턴스 (instance) 범주에 해당하는 구성요소는 1로 설정되고 다른 모든 구성요소는 0으로 설정됩니다.우리의 경우 라벨 $y$은 $(1, 0, 0)$가 “고양이”, $(0, 1, 0)$는 “닭”에 해당하고 $(0, 0, 1)$은 “개”에 해당하는 3 차원 벡터입니다.

$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}.$$

## 네트워크 아키텍처

가능한 모든 클래스와 관련된 조건부 확률을 추정하기 위해 클래스 당 하나씩 여러 출력이있는 모델이 필요합니다.선형 모델을 사용하여 분류를 해결하기 위해 출력이있는만큼 많은 아핀 함수가 필요합니다.각 출력은 자체 아핀 함수에 해당합니다.우리의 경우 4 가지 기능과 3 가지 가능한 출력 범주가 있기 때문에 가중치를 나타내는 12 개의 스칼라 (첨자가있는 $w$) 와 편향을 나타내는 3 개의 스칼라가 필요합니다 (첨자가있는 $b$).각 입력에 대해 다음과 같은 세 가지 *로그*, $o_1, o_2$ 및 $o_3$를 계산합니다.

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$

우리는 :numref:`fig_softmaxreg`에 표시된 신경망 다이어그램으로이 계산을 묘사 할 수 있습니다.선형 회귀 분석에서와 마찬가지로 softmax 회귀 분석도 단일 계층 신경망입니다.그리고 각 출력 $o_1, o_2$ 및 $o_3$의 계산이 모든 입력 ($x_1$, $x_2$, $x_3$ 및 $x_4$) 에 의존하기 때문에 소프트맥스 회귀 분석의 출력 계층도 완전히 연결된 레이어로 설명 될 수 있습니다.

![Softmax regression is a single-layer neural network.](../img/softmaxreg.svg)
:label:`fig_softmaxreg`

모델을 더 콤팩트하게 표현하기 위해 선형 대수 표기법을 사용할 수 있습니다.벡터 형식에서 우리는 $\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$에 도착합니다. 수학과 코드 작성에 더 적합한 형식입니다.우리는 모든 가중치를 $3 \times 4$ 매트릭스로 수집했으며 주어진 데이터 포인트 $\mathbf{x}$의 기능에 대해 우리의 출력은 우리의 입력 기능과 바이어스 $\mathbf{b}$에 의한 가중치의 행렬 벡터 제품에 의해 제공됩니다.

## 소프트맥스 작전

우리가 여기서 취할 주요 접근법은 우리 모델의 결과를 확률로 해석하는 것입니다.관측된 데이터의 가능성을 최대화하는 확률을 생성하기 위해 모수를 최적화할 것입니다.그런 다음 예측을 생성하기 위해 임계 값을 설정합니다 (예: 최대 예측 확률이있는 레이블 선택).

공식적으로 말하면, 우리는 어떤 출력 $\hat{y}_j$를 주어진 항목이 클래스 $j$에 속할 확률로 해석하고 싶습니다.그런 다음 예측 $\operatorname*{argmax}_j y_j$로 가장 큰 출력 값을 가진 클래스를 선택할 수 있습니다.예를 들어 $\hat{y}_1$, $\hat{y}_2$ 및 $\hat{y}_3$이 각각 0.1, 0.8 및 0.1이면 범주 2를 예측합니다. 이 예에서는 “닭”을 나타냅니다.

로짓 $o$를 관심의 결과물로 직접 해석할 것을 제안하고 싶은 유혹을 받을 수 있습니다.그러나 선형 레이어의 출력을 확률로 직접 해석하는 데는 몇 가지 문제가 있습니다.한편으로는 아무것도이 숫자를 합계로 제한하지 않습니다.반면에, 입력에 따라, 그들은 음의 값을 취할 수 있습니다.이들은 :numref:`sec_prob`에 제시된 확률의 기본 공리를 위반합니다.

출력을 확률로 해석하려면 (새 데이터에서도) 음수가 아니고 최대 1의 합계를 보장해야합니다.또한, 우리는 충실한 확률을 추정하기 위해 모델을 장려하는 교육 목표가 필요합니다.분류 자가 0.5를 출력 할 때 모든 인스턴스 중에서 이러한 예제의 절반이 실제로 예측 된 클래스에 속하기를 바랍니다.이것은*보정*이라는 속성입니다.

*의 맥락에서 사회 과학자 R 던컨 루스에 의해 1959 년에 발명 된*softmax 함수*, 정확하게이 않습니다.로짓을 음수가 아니고 합계가 1로 변환하기 위해 모델을 구별 할 수 있도록 요구하면서 먼저 각 로짓을 지수 화 (비 음성을 보장) 한 다음 합계로 나눕니다 (합계가 1로 보장).

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{where}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}. $$
:eqlabel:`eq_softmax_y_and_o`

그것은 모든 $j$에 대한 $0 \leq \hat{y}_j \leq 1$와 함께 $\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$을 쉽게 볼 수 있습니다.따라서 $\hat{\mathbf{y}}$은 요소 값을 그에 따라 해석할 수 있는 적절한 확률 분포입니다.softmax 연산은 각 클래스에 할당 된 확률을 결정하는 사전 소프트 맥스 값인 로짓 $\mathbf{o}$ 간의 순서를 변경하지 않습니다.따라서 예측 중에 우리는 여전히 가장 가능성이 높은 클래스를 선택할 수 있습니다.

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

softmax는 비선형 함수이지만 softmax 회귀 분석의 출력은 입력 피쳐의 아핀 변환에 의해*결정됩니다*. 따라서 softmax 회귀 분석은 선형 모델입니다.

## 미니배치에 대한 벡터화
:label:`subsec_softmax_vectorization`

계산 효율성을 높이고 GPU를 활용하기 위해 일반적으로 데이터의 미니 배치에 대한 벡터 계산을 수행합니다.기능 차원 (입력 수) $d$ 및 배치 크기 $n$을 가진 미니 배치 7323615의 예제가 있다고 가정합니다.또한 출력에 $q$ 범주가 있다고 가정합니다.그런 다음 미니배치 기능 $\mathbf{X}$는 $\mathbb{R}^{n \times d}$에 있고 가중치는 $\mathbf{W} \in \mathbb{R}^{d \times q}$이고 편향은 $\mathbf{b} \in \mathbb{R}^{1\times q}$를 충족합니다.

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
:eqlabel:`eq_minibatch_softmax_reg`

이것은 지배적 인 연산을 행렬 매트릭스 제품 $\mathbf{X} \mathbf{W}$ 대 한 한 한 번에 하나의 예제를 처리 한 경우 실행할 행렬 벡터 제품으로 가속화합니다.$\mathbf{X}$의 각 행은 데이터 포인트를 나타내므로 softmax 연산 자체는* rowwise*를 계산할 수 있습니다. $\mathbf{O}$의 각 행에 대해 모든 항목을 지수 화 한 다음 합계로 정규화합니다.:eqref:`eq_minibatch_softmax_reg`의 합계 $\mathbf{X} \mathbf{W} + \mathbf{b}$ 동안 브로드캐스트를 트리거하는 미니 배치는 모두 $\mathbf{O}$를 기록하고 출력 확률 $\hat{\mathbf{Y}}$는 7323617 행렬입니다.

## 손실 함수

다음으로 예측 확률의 품질을 측정하기 위해 손실 함수가 필요합니다.선형 회귀 분석 (:numref:`subsec_normal_distribution_and_squared_loss`) 에서 평균 제곱 오차 목표에 대한 확률 론적 정당성을 제공 할 때와 동일한 개념 인 최대우도 추정에 의존 할 것입니다.

### 로그 우도

소프트 맥스 함수는 우리에게 우리가 모든 입력 $\mathbf{x}$, 예를 들어, $\hat{y}_1$ = $P(y=\text{cat} \mid \mathbf{x})$ 주어진 각 클래스의 추정 조건부 확률로 해석 할 수있는 벡터 $\hat{\mathbf{y}}$를 제공합니다.전체 데이터 세트 $\{\mathbf{X}, \mathbf{Y}\}$에 $n$ 예제가 있다고 가정합니다. 여기서 $i$로 인덱싱된 예제는 피쳐 벡터 $\mathbf{x}^{(i)}$과 핫 레이블 벡터 $\mathbf{y}^{(i)}$로 구성됩니다.우리는 다음과 같은 특징을 감안할 때 실제 클래스가 모델에 따라 얼마나 가능한지 확인하여 견적을 현실과 비교할 수 있습니다.

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

최대우도 추정에 따라 $P(\mathbf{Y} \mid \mathbf{X})$를 최대화하는데, 이는 음수 로그 우도를 최소화하는 것과 같습니다.

$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$

여기서 $q$ 클래스 이상의 레이블 $\mathbf{y}$ 및 모델 예측 $\hat{\mathbf{y}}$의 경우 손실 함수 $l$는

$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
:eqlabel:`eq_l_cross_entropy`

나중에 설명하는 이유 때문에 :eqref:`eq_l_cross_entropy`의 손실 함수는 일반적으로*교차 엔트로피 손실*이라고합니다.$\mathbf{y}$은 길이가 $q$인 하나의 핫 벡터이기 때문에 모든 좌표 $j$에 대한 합계는 하나의 용어를 제외한 모든 용어에 대해 사라집니다.$\hat{y}_j$은 모두 예측 확률이므로 로그는 $0$보다 크지 않습니다.따라서, 우리가 올바르게* 확실성*, 즉 실제 라벨 $\mathbf{y}$에 대한 예측 확률 7323615 경우 실제 라벨을 예측하면 손실 함수는 더 이상 최소화 될 수 없습니다.이것은 종종 불가능합니다.예를 들어 데이터셋에 레이블 노이즈가 있을 수 있습니다 (일부 예제에서는 레이블이 잘못 지정되었을 수 있음).입력 기능이 모든 예제를 완벽하게 분류하기에 충분한 정보가없는 경우에도 가능하지 않을 수 있습니다.

### 소프트맥스 및 파생 상품
:label:`subsec_softmax_and_derivatives`

softmax와 해당 손실이 매우 일반적이기 때문에 계산 방법을 조금 더 잘 이해할 가치가 있습니다.:eqref:`eq_softmax_y_and_o`를 7323615의 손실의 정의에 연결하고 우리가 얻는 소프트맥스의 정의를 사용하여:

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

무슨 일이 일어나고 있는지 조금 더 잘 이해하려면 모든 로짓 $o_j$와 관련하여 파생물을 고려하십시오.우리는 얻을

$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

즉, 파생 상품은 softmax 연산으로 표현 된대로 모델에 의해 할당 된 확률과 원 핫 레이블 벡터의 요소로 표현 된 것처럼 실제로 일어난 일의 차이입니다.이러한 의미에서 회귀 분석에서 본 것과 매우 유사합니다. 여기서 그라데이션은 관측치 $y$와 $\hat{y}$의 차이입니다.이것은 우연의 일치가 아닙니다.모든 지수 모임 ([online appendix on distributions](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html) 참조) 에서 로그 우도의 그라데이션은 정확하게 이 항으로 계산됩니다.이 사실은 실제로 그라디언트를 쉽게 계산할 수 있습니다.

### 교차 엔트로피 손실

이제 우리는 하나의 결과하지만 결과에 대한 전체 분포뿐만 아니라 관찰 경우를 고려한다.우리는 라벨 $\mathbf{y}$에 대해 이전과 동일한 표현을 사용할 수 있습니다.유일한 차이점은 이진 항목 만 포함하는 벡터가 아니라 $(0, 0, 1)$이라고 말하면 $(0.1, 0.2, 0.7)$라는 일반적인 확률 벡터가 있다는 것입니다.이전에 :eqref:`eq_l_cross_entropy`의 손실 $l$을 정의하기 위해 사용한 수학은 해석이 약간 더 일반적이라는 점에서 여전히 잘 작동합니다.이 레이블을 통해 분포에 대한 손실의 예상 값입니다.이 손실은*교차 엔트로피 손실*이라고하며 분류 문제에 가장 일반적으로 사용되는 손실 중 하나입니다.우리는 정보이론의 기초만을 소개함으로써 그 이름을 이해할 수 있습니다.정보 이론에 대한 자세한 내용을 알고 싶다면 [online appendix on information theory](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)를 참조하십시오.

## 정보 이론 기초

*정보 이론* 인코딩, 디코딩, 전송,
가능한 한 간결한 형식으로 정보 (데이터라고도 함) 를 조작하는 것입니다.

### 엔트로피

정보 이론의 중심 아이디어는 데이터의 정보 내용을 정량화하는 것입니다.이 수량은 데이터를 압축하는 능력에 제한을 둡니다.정보 이론에서 이 양을 분포 $p$의*엔트로피*라고 하며 다음 방정식에 의해 캡처됩니다.

$$H[p] = \sum_j - p(j) \log p(j).$$
:eqlabel:`eq_softmax_reg_entropy`

정보 이론의 기본 정리 중 하나는 $p$ 분포에서 무작위로 가져온 데이터를 인코딩하기 위해이를 인코딩하기 위해 적어도 $H[p]$ “nats”가 필요하다는 것입니다.“nat”이 무엇인지 궁금하다면, 그것은 비트와 동일하지만 기본 2가있는 코드가 아닌 기본 $e$이있는 코드를 사용할 때.따라서 하나의 네이트는 $\frac{1}{\log(2)} \approx 1.44$ 비트입니다.

### 놀라움

압축이 예측과 어떤 관련이 있는지 궁금할 것입니다.압축하려는 데이터 스트림이 있다고 상상해보십시오.다음 토큰을 예측하는 것이 항상 쉽다면이 데이터는 압축하기 쉽습니다!스트림의 모든 토큰이 항상 동일한 값을 취하는 극단적 인 예를 들어 보겠습니다.그것은 매우 지루한 데이터 스트림입니다!그리고 지루할뿐만 아니라 예측하기도 쉽습니다.항상 동일하기 때문에 스트림의 내용을 전달하기 위해 정보를 전송할 필요가 없습니다.예측하기 쉽고 압축하기 쉽습니다.

그러나 모든 사건을 완벽하게 예측할 수 없다면 때때로 놀랄 수도 있습니다.우리가 이벤트를 낮은 확률을 할당 할 때 우리의 놀람은 더 크다.클로드 섀넌은 $j$에게 (주관적) 확률 $P(j)$를 할당 한 이벤트를 관찰에 하나의* 놀라움*을 정량화하기 위해 $\log \frac{1}{P(j)} = -\log P(j)$에 정착했다.:eqref:`eq_softmax_reg_entropy`에 정의 된 엔트로피는 데이터 생성 프로세스와 진정으로 일치하는 올바른 확률을 할당 할 때*예상 놀라움*입니다.

### 교차 엔트로피 재검토

엔트로피가 진정한 확률을 아는 사람이 경험한 놀라움의 수준이라면 교차 엔트로피가 무엇인지 궁금해할 것입니다.$H(p, q)$로 표시된* $p$*에서* $q$의 교차 엔트로피*는 실제로 확률에 따라 생성 된 데이터를 볼 때 주관적 확률 $q$을 가진 관찰자의 예상 놀라움입니다.가장 낮은 가능한 교차 엔트로피는 $p=q$ 때 달성됩니다.이 경우 $p$에서 $q$까지의 교차 엔트로피는 $H(p, p)= H(p)$입니다.

즉, 교차 엔트로피 분류 목표를 두 가지 방법으로 생각할 수 있습니다. (i) 관찰 된 데이터의 가능성을 최대화하는 것으로; (ii) 레이블을 전달하는 데 필요한 놀라움 (따라서 비트 수) 을 최소화하는 것으로 간주합니다.

## 모델 예측 및 평가

softmax 회귀 모델을 훈련 한 후, 예를 들어 기능이 주어지면 각 출력 클래스의 확률을 예측할 수 있습니다.일반적으로 예측 확률이 가장 높은 클래스를 출력 클래스로 사용합니다.예측은 실제 클래스 (레이블) 와 일치하는 경우 정확합니다.실험의 다음 부분에서는*정확성*을 사용하여 모델의 성능을 평가합니다.올바른 예측 수와 총 예측 수 간의 비율과 같습니다.

## 요약

* softmax 연산은 벡터를 가져 와서 확률로 매핑합니다.
* Softmax 회귀 분석은 분류 문제에 적용됩니다.그것은 softmax 동작에서 출력 클래스의 확률 분포를 사용한다.
* 교차 엔트로피는 두 확률 분포 간의 차이에 대한 좋은 척도입니다.그것은 우리의 모델에 주어진 데이터를 인코딩하는 데 필요한 비트 수를 측정합니다.

## 연습 문제

1. 우리는 좀 더 깊이 지수 가족과 softmax 사이의 연결을 탐색 할 수 있습니다.
    * 소프트 맥스에 대한 교차 엔트로피 손실 $l(\mathbf{y},\hat{\mathbf{y}})$의 두 번째 유도체를 계산합니다.
    * $\mathrm{softmax}(\mathbf{o})$에 의해 주어진 분포의 분산을 계산하고 위에서 계산 된 두 번째 유도체와 일치하는지 보여줍니다.
1. 우리는 동일한 확률, 즉, 확률 벡터가 $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$로 발생하는 세 가지 클래스가 있다고 가정합니다.
    * 바이너리 코드를 디자인하려고하면 문제가 무엇입니까?
    * 더 나은 코드를 디자인 할 수 있습니까?힌트: 두 개의 독립적 인 관찰을 인코딩하려고하면 어떻게됩니까?$n$ 관측치를 공동으로 인코딩하면 어떨까요?
1. Softmax는 위에 소개 된 매핑에 대한 잘못된 명목입니다 (그러나 딥 러닝의 모든 사람들이이를 사용합니다).실제 소프트맥스는 $\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$로 정의됩니다.
    * 증명할 수 있는 $\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$.
    * 이 $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b)$에 대한 보유하고 있음을 증명할 수 있습니다.
    * $\lambda \to \infty$에 대해 우리는 $\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$를 가지고 있음을 보여줍니다.
    * 소프트 민은 어떻게 생겼습니까?
    * 이것을 두 개 이상의 숫자로 확장하십시오.

[Discussions](https://discuss.d2l.ai/t/46)
